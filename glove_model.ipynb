{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from glove_batch_generator.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import import_ipynb\n",
    "import glove_batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "learning_rate=0.5\n",
    "window_size = 3\n",
    "vector_len = 50\n",
    "vocab_size = 33702\n",
    "steps = vocab_size**2\n",
    "max_x=100.0\n",
    "alpha=0.75\n",
    "val_indexes = [445,633,4904,16842,1478]\n",
    "#device_name = '/gpu:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove():\n",
    "    Xs = tf.placeholder(tf.int32 , [batch_size,])\n",
    "    Ys = tf.placeholder(tf.int32 , [batch_size,])\n",
    "    cooccur = tf.placeholder (tf.float32 , [batch_size,])\n",
    "    \n",
    "    with tf.variable_scope ('glove') as scope:\n",
    "        Vs = tf.Variable(tf.random_uniform([vocab_size,vector_len] , -1.0 , 1.0))\n",
    "        Us = tf.Variable(tf.random_uniform([vocab_size,vector_len] , -1.0 , 1.0))\n",
    "        \n",
    "        V_bias = tf.Variable(tf.random_uniform([vocab_size] , -1.0 , 1.0))\n",
    "        U_bias = tf.Variable(tf.random_uniform([vocab_size] , -1.0 , 1.0))\n",
    "        \n",
    "        X_embd = tf.nn.embedding_lookup(Vs , Xs)\n",
    "        Y_embd = tf.nn.embedding_lookup(Us , Ys)\n",
    "        \n",
    "        X_bias = tf.nn.embedding_lookup(V_bias , Xs)\n",
    "        Y_bias = tf.nn.embedding_lookup(U_bias , Ys)\n",
    "        \n",
    "        fs = tf.minimum (1.0 , tf.pow(tf.div(tf.to_float(cooccur) , max_x) ,alpha))\n",
    "        \n",
    "        log_cooccur = tf.log((cooccur))\n",
    "        \n",
    "        prod = tf.reduce_sum (tf.multiply(X_embd , Y_embd), axis=1)\n",
    "        \n",
    "        \n",
    "        dist = tf.square(tf.add_n([prod , X_bias , Y_bias , (log_cooccur)]))\n",
    "        \n",
    "        loss = tf.multiply(fs , dist)\n",
    "        \n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        new_embd = tf.add(Us , Vs)/2\n",
    "        \n",
    "        val_embd = tf.identity(new_embd)\n",
    "        similarity = tf.matmul(val_embd , new_embd , transpose_b=True)\n",
    "        \n",
    "        return Xs , Ys , cooccur  ,  total_loss , new_embd , similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    yield from glove_batch_generator.gen_batch(window_size , batch_size , vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    with open(\"indexWords.txt\" , \"rt\") as infile:\n",
    "        index_words = eval(infile.read())\n",
    "        \n",
    "        \n",
    "    data = tf.data.Dataset.from_generator(generator , (tf.int32 , tf.int32 , tf.int32))\n",
    "    iterator = data.make_initializable_iterator()\n",
    "    first , second , cooc = iterator.get_next()\n",
    "    \n",
    "  \n",
    "   # with tf.device(device_name):\n",
    "    Xs , Ys , cooccur , total_loss  , new_embd , similarity = glove()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(iterator.initializer)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range (steps):\n",
    "            try:\n",
    "                batch_loss , _ = sess.run([ total_loss , optimizer] , feed_dict={Xs: first.eval() , Ys: second.eval() , cooccur: cooc.eval() })\n",
    "                    \n",
    "                if i%1000 ==0:\n",
    "                    print(str(i)+\":\"+str(batch_loss))\n",
    "                \n",
    "\n",
    "                if i%5000 == 0:\n",
    "                    sim = similarity.eval()\n",
    "\n",
    "                    for j in val_indexes:\n",
    "                        listt = (-sim[j,:]).argsort()[1:6]\n",
    "                        print(listt)\n",
    "                        print (index_words[j])\n",
    "                        list2 = [index_words[k] for k in listt]\n",
    "                        print (list2)\n",
    "\n",
    "\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sess.run(iterator.initializer)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.3832074\n",
      "[13708 10279   633 20304 15471]\n",
      "two\n",
      "['natufian', 'unfair', 'person', 'regiment', 'uc']\n",
      "[10279 30121   884 27313 17074]\n",
      "person\n",
      "['unfair', 'sadler', 'fail', 'deschauenseei', 'footed']\n",
      "[ 6197  7558 11149 21824 10108]\n",
      "england\n",
      "['realm', 'baruch', 'crunches', 'vectors', 'burials']\n",
      "[31497 33219   538 29730 17107]\n",
      "week\n",
      "['procure', 'reassuring', 'bakunin', 'stonewall', 'conveyor']\n",
      "[ 2911 27949 31718 32150 28170]\n",
      "big\n",
      "['hypothesis', 'hanseatic', 'galaxy', 'unicorn', 'intervehicle']\n",
      "1000:0.2602082\n",
      "2000:0.16086784\n",
      "3000:0.20905596\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
